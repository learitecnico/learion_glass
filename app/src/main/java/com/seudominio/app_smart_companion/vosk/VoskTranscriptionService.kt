package com.seudominio.app_smart_companion.vosk

import android.content.Context
import android.os.Handler
import android.os.Looper
import android.util.Log
import org.json.JSONException
import org.json.JSONObject
import org.vosk.LibVosk
import org.vosk.LogLevel
import org.vosk.Model
import org.vosk.Recognizer
import org.vosk.android.RecognitionListener
import org.vosk.android.StorageService
import java.util.concurrent.ArrayBlockingQueue
import java.util.concurrent.BlockingQueue

/**
 * Vosk local speech recognition service
 * Adapted from SmartGlassesManager implementation for Coach SPIN integration
 */
class VoskTranscriptionService(
    private val context: Context,
    private val callback: TranscriptionCallback
) : RecognitionListener {

    companion object {
        private const val TAG = "VoskTranscriptionService"
        private const val SAMPLE_RATE = 16000.0f
        private const val LANGUAGE_MODEL = "model-pt-br"  // Portugu√™s brasileiro
        private const val FALLBACK_MODEL = "model-en-us"  // Fallback para ingl√™s
        private const val AUDIO_BUFFER_SIZE = (16000 * 2 * 0.192).toInt() // ~192ms buffer
    }

    interface TranscriptionCallback {
        fun onTranscriptionResult(text: String, isFinal: Boolean)
        fun onTranscriptionError(error: String)
        fun onServiceReady()
    }

    // Vosk components
    private var model: Model? = null
    private var recognizer: Recognizer? = null
    private var speechStreamService: SpeechStreamQueueServiceVosk? = null
    
    // Audio processing
    private val audioQueue: BlockingQueue<ByteArray> = ArrayBlockingQueue(AUDIO_BUFFER_SIZE)
    private val mainHandler = Handler(Looper.getMainLooper())
    
    // State management
    private var isInitialized = false
    private var isListening = false

    /**
     * Initialize Vosk service and load model
     */
    fun initialize() {
        Log.d(TAG, "üé§ Initializing Vosk transcription service...")
        
        // Set Vosk log level
        LibVosk.setLogLevel(LogLevel.INFO)
        
        // Load model
        initModel()
        
        // Start recognition after delay (wait for model loading)
        val delay = 500L
        mainHandler.postDelayed({
            if (model != null) {
                Log.d(TAG, "‚úÖ Vosk model loaded, ready for transcription")
                isInitialized = true
                callback.onServiceReady()
            } else {
                // Retry if model not loaded yet
                mainHandler.postDelayed({ initialize() }, delay)
            }
        }, delay)
    }

    /**
     * Load Vosk model from assets
     */
    private fun initModel() {
        Log.d(TAG, "üì¶ Loading Vosk model: $LANGUAGE_MODEL")
        
        try {
            // Try Portuguese model first - directly from assets folder
            val modelPath = "file:///android_asset/$LANGUAGE_MODEL"
            
            try {
                model = Model(modelPath)
                Log.d(TAG, "‚úÖ Vosk Portuguese model loaded successfully from: $modelPath")
                return
            } catch (e: Exception) {
                Log.w(TAG, "‚ö†Ô∏è Portuguese model failed to load: ${e.message}")
                
                // Fallback to English model
                val fallbackPath = "file:///android_asset/$FALLBACK_MODEL"
                try {
                    model = Model(fallbackPath)
                    Log.d(TAG, "‚úÖ Vosk English fallback model loaded successfully from: $fallbackPath")
                    return
                } catch (fe: Exception) {
                    val errorMsg = "‚ùå Failed to load any Vosk model. PT-BR: ${e.message}, EN: ${fe.message}"
                    Log.e(TAG, errorMsg)
                    callback.onTranscriptionError(errorMsg)
                }
            }
            
        } catch (e: Exception) {
            val errorMsg = "‚ùå Exception loading Vosk model: ${e.message}"
            Log.e(TAG, errorMsg)
            callback.onTranscriptionError(errorMsg)
        }
    }

    /**
     * Start listening for audio input
     */
    fun startListening() {
        if (!isInitialized) {
            Log.w(TAG, "‚ö†Ô∏è Service not initialized yet")
            return
        }
        
        if (isListening) {
            Log.w(TAG, "‚ö†Ô∏è Already listening")
            return
        }

        Log.d(TAG, "üé§ Starting Vosk recognition...")
        
        val currentModel = model
        if (currentModel == null) {
            val errorMsg = "‚ùå Vosk model not loaded"
            Log.e(TAG, errorMsg)
            callback.onTranscriptionError(errorMsg)
            return
        }

        // NOVA ABORDAGEM: Bypass Vosk - enviar WAV diretamente para OpenAI
        Log.d(TAG, "üåê Enviando √°udio WAV diretamente para OpenAI Whisper via Companion")
        
        try {
            // Notificar que come√ßamos a "ouvir" (processar √°udio)
            isListening = true
            
            // Em vez de usar Vosk, vamos processar o arquivo de √°udio diretamente
            // e enviar para OpenAI via WebSocket (j√° existente)
            Log.d(TAG, "‚úÖ Modo OpenAI Whisper ativo - processando arquivo de √°udio")
            
            // O √°udio ser√° processado pelo CoachAudioRecorder e enviado via WebSocket
            // para o Companion Desktop, que far√° a transcri√ß√£o via OpenAI Whisper
            
        } catch (e: Exception) {
            val errorMsg = "‚ùå Failed to start OpenAI audio processing: ${e.message}"
            Log.e(TAG, errorMsg, e)
            callback.onTranscriptionError(errorMsg)
            isListening = false
        }
    }

    /**
     * Stop listening for audio input
     */
    fun stopListening() {
        if (!isListening) {
            Log.w(TAG, "‚ö†Ô∏è Not currently listening")
            return
        }

        Log.d(TAG, "üõë Stopping Vosk recognition...")
        
        speechStreamService?.stop()
        speechStreamService = null
        recognizer = null
        isListening = false
        
        Log.d(TAG, "‚úÖ Vosk recognition stopped")
    }

    /**
     * Process audio chunk - now sends directly to OpenAI via WebSocket
     */
    fun processAudioChunk(audioData: ByteArray) {
        if (!isListening) {
            Log.v(TAG, "‚ö†Ô∏è Not listening, ignoring audio chunk")
            return
        }

        try {
            // NOVA ABORDAGEM: Em vez de Vosk, acumular √°udio para envio via WebSocket
            Log.v(TAG, "üåê Audio chunk para OpenAI Whisper (${audioData.size} bytes)")
            
            // TODO: Implementar envio via WebSocket para Companion Desktop
            // O Companion Desktop usar√° OpenAI Whisper API para transcri√ß√£o
            
        } catch (e: Exception) {
            Log.w(TAG, "‚ö†Ô∏è Error processing audio chunk: ${e.message}")
        }
    }

    /**
     * Clean up resources
     */
    fun destroy() {
        Log.d(TAG, "üßπ Destroying Vosk service...")
        
        stopListening()
        model = null
        isInitialized = false
        
        Log.d(TAG, "‚úÖ Vosk service destroyed")
    }

    // RecognitionListener implementation
    override fun onResult(hypothesis: String) {
        mainHandler.post {
            handleTranscriptionResult(hypothesis, isFinal = true)
        }
    }

    override fun onPartialResult(hypothesis: String) {
        mainHandler.post {
            handleTranscriptionResult(hypothesis, isFinal = false)
        }
    }

    override fun onFinalResult(hypothesis: String) {
        Log.d(TAG, "üéØ Final result received")
        // Speech service completed, ready for next input
    }

    override fun onError(exception: Exception) {
        val errorMsg = "‚ùå Vosk recognition error: ${exception.message}"
        Log.e(TAG, errorMsg)
        mainHandler.post {
            callback.onTranscriptionError(errorMsg)
        }
    }

    override fun onTimeout() {
        Log.d(TAG, "‚è∞ Vosk recognition timeout")
        // This is normal behavior, not an error
    }

    /**
     * Handle transcription result from Vosk
     */
    private fun handleTranscriptionResult(hypothesis: String, isFinal: Boolean) {
        try {
            val voskResponse = JSONObject(hypothesis)
            val transcript = if (isFinal) {
                voskResponse.optString("text", "")
            } else {
                voskResponse.optString("partial", "")
            }

            // Filter out empty or invalid transcripts
            if (transcript.isBlank()) {
                Log.v(TAG, "üìù Empty transcript, ignoring")
                return
            }

            // Filter out common false positives
            val badWords = arrayOf("huh", "her", "hit", "cut", "if", "but", "by", "hi", "ha", "a", "the", "it")
            if (transcript.trim().lowercase() in badWords) {
                Log.v(TAG, "üìù False positive transcript filtered: '$transcript'")
                return
            }

            Log.d(TAG, "üìù Transcript ${if (isFinal) "FINAL" else "PARTIAL"}: '$transcript'")
            callback.onTranscriptionResult(transcript, isFinal)

        } catch (e: JSONException) {
            Log.e(TAG, "‚ùå Failed to parse Vosk response: ${e.message}")
            callback.onTranscriptionError("Failed to parse transcription result")
        }
    }
}